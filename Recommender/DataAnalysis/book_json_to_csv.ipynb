{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this file is for changing the json file to csv file\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = 'C:/Files/College/GP/UCSDBooks/Downloads/goodreads_reviews_dedup.json'\n",
    "csv_path = '../preprocessed_original/goodreads_books.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'user_id': '8842281e1d1347389f2ab93d60773d4d',                                             \n",
    "#  'book_id': '4986701',                                                                      \n",
    "#  'review_id': 'bb7de32f9fadc36627e61aaef7a93142',                                           \n",
    "#  'rating': 4,                                                                               \n",
    "#  'review_text': 'Found the Goodreads down image in this, and many other useful images too!',\n",
    "#  'date_added': 'Thu Aug 04 10:02:02 -0700 2011',                                            \n",
    "#  'date_updated': 'Thu Aug 04 10:02:02 -0700 2011',                                          \n",
    "#  'read_at': '',                                                                             \n",
    "#  'started_at': '',                                                                          \n",
    "#  'n_votes': 6,                                                                              \n",
    "#  'n_comments': 4}                                                                           \n",
    "\n",
    "\n",
    "# this is the list of the columns we want to keep from the json file\n",
    "columns = ['user_id','book_id','rating','review_text']\n",
    "json_data = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished reading 100000 lines in 24.96824812889099 seconds\n",
      "finished reading 200000 lines in 2.21850323677063 seconds\n",
      "finished reading 300000 lines in 26.216259002685547 seconds\n",
      "finished reading 400000 lines in 24.28700041770935 seconds\n",
      "finished reading 500000 lines in 5.369115114212036 seconds\n",
      "finished reading 600000 lines in 26.21747589111328 seconds\n",
      "finished reading 700000 lines in 14.089137554168701 seconds\n",
      "finished reading 800000 lines in 4.634800434112549 seconds\n",
      "finished reading 900000 lines in 25.484811067581177 seconds\n",
      "finished reading 1000000 lines in 14.351242303848267 seconds\n",
      "finished reading 1100000 lines in 12.180737972259521 seconds\n",
      "finished reading 1200000 lines in 36.805734157562256 seconds\n",
      "finished reading 1300000 lines in 3.5180299282073975 seconds\n",
      "finished reading 1400000 lines in 27.977418422698975 seconds\n",
      "finished reading 1500000 lines in 13.430569648742676 seconds\n",
      "finished reading 1600000 lines in 9.726815938949585 seconds\n",
      "finished reading 1700000 lines in 4.086557388305664 seconds\n",
      "finished reading 1800000 lines in 5.305198907852173 seconds\n",
      "finished reading 1900000 lines in 3.536705732345581 seconds\n",
      "finished reading 2000000 lines in 24.458180904388428 seconds\n",
      "finished reading 2100000 lines in 12.496235132217407 seconds\n",
      "finished reading 2200000 lines in 23.70129680633545 seconds\n",
      "finished reading 2300000 lines in 24.62173843383789 seconds\n",
      "finished reading 2400000 lines in 36.47586750984192 seconds\n",
      "finished reading 2500000 lines in 26.402903079986572 seconds\n",
      "finished reading 2600000 lines in 25.657596588134766 seconds\n",
      "finished reading 2700000 lines in 25.380987882614136 seconds\n",
      "finished reading 2800000 lines in 14.744715690612793 seconds\n",
      "finished reading 2900000 lines in 25.23803973197937 seconds\n",
      "finished reading 3000000 lines in 14.03629207611084 seconds\n",
      "finished reading 3100000 lines in 24.20436143875122 seconds\n",
      "finished reading 3200000 lines in 22.710744380950928 seconds\n",
      "finished reading 3300000 lines in 5.607189893722534 seconds\n",
      "finished reading 3400000 lines in 23.27006983757019 seconds\n",
      "finished reading 3500000 lines in 37.45928168296814 seconds\n",
      "finished reading 3600000 lines in 23.188551902770996 seconds\n",
      "finished reading 3700000 lines in 25.716627597808838 seconds\n",
      "finished reading 3800000 lines in 41.20286464691162 seconds\n",
      "finished reading 3900000 lines in 26.34171199798584 seconds\n",
      "finished reading 4000000 lines in 24.721843481063843 seconds\n",
      "finished reading 4100000 lines in 9.874219179153442 seconds\n",
      "finished reading 4200000 lines in 13.208970785140991 seconds\n",
      "finished reading 4300000 lines in 12.99655270576477 seconds\n",
      "finished reading 4400000 lines in 12.989994764328003 seconds\n",
      "finished reading 4500000 lines in 3.986377239227295 seconds\n",
      "finished reading 4600000 lines in 9.766392230987549 seconds\n",
      "finished reading 4700000 lines in 24.18732786178589 seconds\n",
      "finished reading 4800000 lines in 14.426238059997559 seconds\n",
      "finished reading 4900000 lines in 26.031920194625854 seconds\n",
      "finished reading 5000000 lines in 26.941587924957275 seconds\n",
      "finished reading 5100000 lines in 8.357927560806274 seconds\n",
      "finished reading 5200000 lines in 27.33318257331848 seconds\n",
      "finished reading 5300000 lines in 4.8767640590667725 seconds\n",
      "finished reading 5400000 lines in 2.602029800415039 seconds\n",
      "finished reading 5500000 lines in 24.357887983322144 seconds\n",
      "finished reading 5600000 lines in 29.148701667785645 seconds\n",
      "finished reading 5700000 lines in 25.39157199859619 seconds\n",
      "finished reading 5800000 lines in 24.03409504890442 seconds\n",
      "finished reading 5900000 lines in 24.37240743637085 seconds\n",
      "finished reading 6000000 lines in 4.527389287948608 seconds\n",
      "finished reading 6100000 lines in 25.977123022079468 seconds\n",
      "finished reading 6200000 lines in 11.161801815032959 seconds\n",
      "finished reading 6300000 lines in 13.14631700515747 seconds\n",
      "finished reading 6400000 lines in 25.526506423950195 seconds\n",
      "finished reading 6500000 lines in 40.691301345825195 seconds\n",
      "finished reading 6600000 lines in 14.569421291351318 seconds\n",
      "finished reading 6700000 lines in 1.865281581878662 seconds\n",
      "finished reading 6800000 lines in 40.78066945075989 seconds\n",
      "finished reading 6900000 lines in 5.413367986679077 seconds\n",
      "finished reading 7000000 lines in 26.594514846801758 seconds\n",
      "finished reading 7100000 lines in 27.836081981658936 seconds\n",
      "finished reading 7200000 lines in 27.225428104400635 seconds\n",
      "finished reading 7300000 lines in 3.6663615703582764 seconds\n",
      "finished reading 7400000 lines in 40.28131890296936 seconds\n",
      "finished reading 7500000 lines in 13.335868835449219 seconds\n",
      "finished reading 7600000 lines in 13.874892950057983 seconds\n",
      "finished reading 7700000 lines in 24.720932960510254 seconds\n",
      "finished reading 7800000 lines in 23.81979012489319 seconds\n",
      "finished reading 7900000 lines in 23.683261156082153 seconds\n",
      "finished reading 8000000 lines in 30.791906118392944 seconds\n",
      "finished reading 8100000 lines in 38.89849281311035 seconds\n",
      "finished reading 8200000 lines in 3.5298478603363037 seconds\n",
      "finished reading 8300000 lines in 13.527544736862183 seconds\n",
      "finished reading 8400000 lines in 4.2811126708984375 seconds\n",
      "finished reading 8500000 lines in 11.270777940750122 seconds\n",
      "finished reading 8600000 lines in 26.097097396850586 seconds\n",
      "finished reading 8700000 lines in 27.15052080154419 seconds\n",
      "finished reading 8800000 lines in 25.788859605789185 seconds\n",
      "finished reading 8900000 lines in 9.947330474853516 seconds\n",
      "finished reading 9000000 lines in 25.04794454574585 seconds\n",
      "finished reading 9100000 lines in 26.951133489608765 seconds\n",
      "finished reading 9200000 lines in 13.607077836990356 seconds\n",
      "finished reading 9300000 lines in 42.376983404159546 seconds\n",
      "finished reading 9400000 lines in 13.241927862167358 seconds\n",
      "finished reading 9500000 lines in 9.825364112854004 seconds\n",
      "finished reading 9600000 lines in 14.034342527389526 seconds\n",
      "finished reading 9700000 lines in 25.080790519714355 seconds\n",
      "finished reading 9800000 lines in 10.430870771408081 seconds\n",
      "finished reading 9900000 lines in 12.742181301116943 seconds\n",
      "finished reading 10000000 lines in 5.881723165512085 seconds\n",
      "finished reading 10100000 lines in 5.92420220375061 seconds\n",
      "finished reading 10200000 lines in 3.190314769744873 seconds\n",
      "finished reading 10300000 lines in 42.46949291229248 seconds\n",
      "finished reading 10400000 lines in 6.6551597118377686 seconds\n",
      "finished reading 10500000 lines in 13.62771987915039 seconds\n",
      "finished reading 10600000 lines in 4.704228162765503 seconds\n",
      "finished reading 10700000 lines in 42.57466006278992 seconds\n",
      "finished reading 10800000 lines in 29.994906663894653 seconds\n",
      "finished reading 10900000 lines in 5.846056699752808 seconds\n",
      "finished reading 11000000 lines in 43.791412353515625 seconds\n",
      "finished reading 11100000 lines in 9.644011497497559 seconds\n",
      "finished reading 11200000 lines in 3.86637282371521 seconds\n",
      "finished reading 11300000 lines in 5.297523736953735 seconds\n",
      "finished reading 11400000 lines in 17.4189236164093 seconds\n",
      "finished reading 11500000 lines in 22.85939121246338 seconds\n",
      "finished reading 11600000 lines in 26.884398221969604 seconds\n",
      "finished reading 11700000 lines in 4.388486623764038 seconds\n",
      "finished reading 11800000 lines in 14.555225133895874 seconds\n",
      "finished reading 11900000 lines in 37.99703764915466 seconds\n",
      "finished reading 12000000 lines in 26.073060274124146 seconds\n",
      "finished reading 12100000 lines in 25.721314668655396 seconds\n",
      "finished reading 12200000 lines in 7.508661985397339 seconds\n",
      "finished reading 12300000 lines in 23.907505750656128 seconds\n",
      "finished reading 12400000 lines in 3.6947035789489746 seconds\n",
      "finished reading 12500000 lines in 11.15480375289917 seconds\n",
      "finished reading 12600000 lines in 7.583031177520752 seconds\n",
      "finished reading 12700000 lines in 37.68914079666138 seconds\n",
      "finished reading 12800000 lines in 26.34283447265625 seconds\n",
      "finished reading 12900000 lines in 26.94105577468872 seconds\n",
      "finished reading 13000000 lines in 41.02591848373413 seconds\n",
      "finished reading 13100000 lines in 24.261979341506958 seconds\n",
      "finished reading 13200000 lines in 4.006807804107666 seconds\n",
      "finished reading 13300000 lines in 7.716086387634277 seconds\n",
      "finished reading 13400000 lines in 38.23559761047363 seconds\n",
      "finished reading 13500000 lines in 25.955233812332153 seconds\n",
      "finished reading 13600000 lines in 26.176437616348267 seconds\n",
      "finished reading 13700000 lines in 6.000700950622559 seconds\n",
      "finished reading 13800000 lines in 43.0373592376709 seconds\n",
      "finished reading 13900000 lines in 13.197133541107178 seconds\n",
      "finished reading 14000000 lines in 23.66939115524292 seconds\n",
      "finished reading 14100000 lines in 13.279646635055542 seconds\n",
      "finished reading 14200000 lines in 24.833184242248535 seconds\n",
      "finished reading 14300000 lines in 23.61699390411377 seconds\n",
      "finished reading 14400000 lines in 6.883924961090088 seconds\n",
      "finished reading 14500000 lines in 2.6613929271698 seconds\n",
      "finished reading 14600000 lines in 24.83199453353882 seconds\n",
      "finished reading 14700000 lines in 12.999493837356567 seconds\n",
      "finished reading 14800000 lines in 25.040765523910522 seconds\n",
      "finished reading 14900000 lines in 6.106500625610352 seconds\n",
      "finished reading 15000000 lines in 4.596349239349365 seconds\n",
      "finished reading 15100000 lines in 3.4837186336517334 seconds\n",
      "finished reading 15200000 lines in 1.9863831996917725 seconds\n",
      "finished reading 15300000 lines in 1.9044675827026367 seconds\n",
      "finished reading 15400000 lines in 4.643296957015991 seconds\n",
      "finished reading 15500000 lines in 1.6765780448913574 seconds\n",
      "finished reading 15600000 lines in 4.147678375244141 seconds\n",
      "finished reading 15700000 lines in 1.9662632942199707 seconds\n",
      "finished reading 15800000 lines in 1.733384370803833 seconds\n"
     ]
    }
   ],
   "source": [
    "# reading large data from json file using pandas\n",
    "chunk_size = 100000\n",
    "count = 0 \n",
    "time_end = time.time()\n",
    "for chunk in pd.read_json(json_path, lines=True, chunksize=chunk_size):\n",
    "    # for the authors column, we only want the first author id\n",
    "    # check if the authors column is not empty\n",
    "    # calclate time for each chunk\n",
    "    time_start = time_end\n",
    "    # only keep the rest of the columns we want\n",
    "    chunk = chunk[columns]\n",
    "    # append the chunk to the json_data list\n",
    "    json_data.append(chunk)\n",
    "    \n",
    "    # print the progress\n",
    "    count += chunk_size\n",
    "    del chunk\n",
    "    time_end = time.time()\n",
    "    print(\"finished reading {} lines in {} seconds\".format(count, time_end - time_start))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the list of dataframes into one dataframe\n",
    "df = pd.concat(json_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2080190\n"
     ]
    }
   ],
   "source": [
    "# save the dataframe to csv file\n",
    "# print unique values of book_id \n",
    "print(len(df['book_id'].unique()))\n",
    "df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merging all the chunks into one json file\n",
    "# json_data = pd.concat(json_data, ignore_index=True)\n",
    "# print(json_data.shape)\n",
    "# # saving the json file to csv file\n",
    "# json_data.to_csv(csv_path, index=False)\n",
    "# find book with ID = 7327624\n",
    "# print(json_data[json_data['book_id'] == '7327624'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>24375664</td>\n",
       "      <td>5</td>\n",
       "      <td>Mind blowingly cool. Best science fiction I've...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>18245960</td>\n",
       "      <td>5</td>\n",
       "      <td>This is a special book. It started slow for ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>6392944</td>\n",
       "      <td>3</td>\n",
       "      <td>I haven't read a fun mystery book in a while a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>22078596</td>\n",
       "      <td>4</td>\n",
       "      <td>Fun, fast paced, and disturbing tale of murder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>6644782</td>\n",
       "      <td>4</td>\n",
       "      <td>A fun book that gives you a sense of living in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            user_id   book_id  rating  \\\n",
       "0  8842281e1d1347389f2ab93d60773d4d  24375664       5   \n",
       "1  8842281e1d1347389f2ab93d60773d4d  18245960       5   \n",
       "2  8842281e1d1347389f2ab93d60773d4d   6392944       3   \n",
       "3  8842281e1d1347389f2ab93d60773d4d  22078596       4   \n",
       "4  8842281e1d1347389f2ab93d60773d4d   6644782       4   \n",
       "\n",
       "                                         review_text  \n",
       "0  Mind blowingly cool. Best science fiction I've...  \n",
       "1  This is a special book. It started slow for ab...  \n",
       "2  I haven't read a fun mystery book in a while a...  \n",
       "3  Fun, fast paced, and disturbing tale of murder...  \n",
       "4  A fun book that gives you a sense of living in...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the csv file\n",
    "df = pd.read_csv(csv_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find a book with multiple series\n",
    "# df[df['series'].str.contains(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
