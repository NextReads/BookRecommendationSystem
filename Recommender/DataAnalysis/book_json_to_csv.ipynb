{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this file is for changing the json file to csv file\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = 'C:/Files/College/GP/UCSDBooks/Downloads/goodreads_reviews_dedup.json'\n",
    "csv_path = '../preprocessed_original/goodreads_books.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'user_id': '8842281e1d1347389f2ab93d60773d4d',                                             \n",
    "#  'book_id': '4986701',                                                                      \n",
    "#  'review_id': 'bb7de32f9fadc36627e61aaef7a93142',                                           \n",
    "#  'rating': 4,                                                                               \n",
    "#  'review_text': 'Found the Goodreads down image in this, and many other useful images too!',\n",
    "#  'date_added': 'Thu Aug 04 10:02:02 -0700 2011',                                            \n",
    "#  'date_updated': 'Thu Aug 04 10:02:02 -0700 2011',                                          \n",
    "#  'read_at': '',                                                                             \n",
    "#  'started_at': '',                                                                          \n",
    "#  'n_votes': 6,                                                                              \n",
    "#  'n_comments': 4}                                                                           \n",
    "\n",
    "\n",
    "# this is the list of the columns we want to keep from the json file\n",
    "columns = ['book_id', 'user_id', 'rating']\n",
    "json_data = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished reading 100000 lines in 24.433274745941162 seconds\n",
      "finished reading 200000 lines in 1.818023681640625 seconds\n",
      "finished reading 300000 lines in 25.25330352783203 seconds\n",
      "finished reading 400000 lines in 25.642704486846924 seconds\n",
      "finished reading 500000 lines in 5.567541599273682 seconds\n",
      "finished reading 600000 lines in 25.51956057548523 seconds\n",
      "finished reading 700000 lines in 15.53979754447937 seconds\n",
      "finished reading 800000 lines in 6.654128551483154 seconds\n",
      "finished reading 900000 lines in 25.812649965286255 seconds\n",
      "finished reading 1000000 lines in 12.938563346862793 seconds\n",
      "finished reading 1100000 lines in 11.94138765335083 seconds\n",
      "finished reading 1200000 lines in 43.7159628868103 seconds\n",
      "finished reading 1300000 lines in 4.889546871185303 seconds\n",
      "finished reading 1400000 lines in 28.396724462509155 seconds\n",
      "finished reading 1500000 lines in 13.132951736450195 seconds\n",
      "finished reading 1600000 lines in 9.92030668258667 seconds\n",
      "finished reading 1700000 lines in 4.164427042007446 seconds\n",
      "finished reading 1800000 lines in 5.570641994476318 seconds\n",
      "finished reading 1900000 lines in 3.558544635772705 seconds\n",
      "finished reading 2000000 lines in 27.434911012649536 seconds\n",
      "finished reading 2100000 lines in 13.762328147888184 seconds\n",
      "finished reading 2200000 lines in 25.1159610748291 seconds\n",
      "finished reading 2300000 lines in 26.583840131759644 seconds\n",
      "finished reading 2400000 lines in 37.15762662887573 seconds\n",
      "finished reading 2500000 lines in 26.623112201690674 seconds\n",
      "finished reading 2600000 lines in 35.25368285179138 seconds\n",
      "finished reading 2700000 lines in 28.95303463935852 seconds\n",
      "finished reading 2800000 lines in 13.102984428405762 seconds\n",
      "finished reading 2900000 lines in 28.82138752937317 seconds\n",
      "finished reading 3000000 lines in 17.24748945236206 seconds\n",
      "finished reading 3100000 lines in 32.44881319999695 seconds\n",
      "finished reading 3200000 lines in 31.604480743408203 seconds\n",
      "finished reading 3300000 lines in 8.227797746658325 seconds\n",
      "finished reading 3400000 lines in 31.611887454986572 seconds\n",
      "finished reading 3500000 lines in 43.269928216934204 seconds\n",
      "finished reading 3600000 lines in 25.7542827129364 seconds\n",
      "finished reading 3700000 lines in 23.602354526519775 seconds\n",
      "finished reading 3800000 lines in 36.8091459274292 seconds\n",
      "finished reading 3900000 lines in 26.125814199447632 seconds\n",
      "finished reading 4000000 lines in 25.10396432876587 seconds\n",
      "finished reading 4100000 lines in 9.792306184768677 seconds\n",
      "finished reading 4200000 lines in 12.536128997802734 seconds\n",
      "finished reading 4300000 lines in 14.402771949768066 seconds\n",
      "finished reading 4400000 lines in 9.93180799484253 seconds\n",
      "finished reading 4500000 lines in 3.9786200523376465 seconds\n",
      "finished reading 4600000 lines in 7.703604698181152 seconds\n",
      "finished reading 4700000 lines in 22.922990560531616 seconds\n",
      "finished reading 4800000 lines in 13.249069929122925 seconds\n",
      "finished reading 4900000 lines in 26.476232290267944 seconds\n",
      "finished reading 5000000 lines in 26.236414194107056 seconds\n",
      "finished reading 5100000 lines in 7.045695543289185 seconds\n",
      "finished reading 5200000 lines in 25.758995294570923 seconds\n",
      "finished reading 5300000 lines in 4.3281028270721436 seconds\n",
      "finished reading 5400000 lines in 1.7706243991851807 seconds\n",
      "finished reading 5500000 lines in 24.841617107391357 seconds\n",
      "finished reading 5600000 lines in 24.266278743743896 seconds\n",
      "finished reading 5700000 lines in 27.37067484855652 seconds\n",
      "finished reading 5800000 lines in 24.820632934570312 seconds\n",
      "finished reading 5900000 lines in 25.126561164855957 seconds\n",
      "finished reading 6000000 lines in 4.386642217636108 seconds\n",
      "finished reading 6100000 lines in 26.32133173942566 seconds\n",
      "finished reading 6200000 lines in 12.16923213005066 seconds\n",
      "finished reading 6300000 lines in 14.485609531402588 seconds\n"
     ]
    }
   ],
   "source": [
    "# reading large data from json file using pandas\n",
    "chunk_size = 100000\n",
    "count = 0 \n",
    "time_end = time.time()\n",
    "for chunk in pd.read_json(json_path, lines=True, chunksize=chunk_size):\n",
    "    # for the authors column, we only want the first author id\n",
    "    # check if the authors column is not empty\n",
    "    # calclate time for each chunk\n",
    "    time_start = time_end\n",
    "    # only keep the rest of the columns we want\n",
    "    chunk = chunk[columns]\n",
    "    # append the chunk to the json_data list\n",
    "    json_data.append(chunk)\n",
    "    \n",
    "    # print the progress\n",
    "    count += chunk_size\n",
    "    del chunk\n",
    "    time_end = time.time()\n",
    "    print(\"finished reading {} lines in {} seconds\".format(count, time_end - time_start))\n",
    "\n",
    "# vectorize the previous for loop\n",
    "json_data = [chunk[columns] for chunk in pd.read_json(json_path, lines=True, chunksize=chunk_size)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the list of dataframes into one dataframe\n",
    "df = pd.concat(json_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2360655\n"
     ]
    }
   ],
   "source": [
    "# save the dataframe to csv file\n",
    "# print unique values of book_id \n",
    "print(len(df['book_id'].unique()))\n",
    "df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merging all the chunks into one json file\n",
    "# json_data = pd.concat(json_data, ignore_index=True)\n",
    "# print(json_data.shape)\n",
    "# # saving the json file to csv file\n",
    "# json_data.to_csv(csv_path, index=False)\n",
    "# find book with ID = 7327624\n",
    "# print(json_data[json_data['book_id'] == '7327624'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>work_id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>series</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>language_code</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>text_reviews_count</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>image_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5333265</td>\n",
       "      <td>5400751.0</td>\n",
       "      <td>0312853122</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W.C. Fields: A Life on Film</td>\n",
       "      <td>604031.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1984.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>https://images.gr-assets.com/books/1310220028m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1333909</td>\n",
       "      <td>1323437.0</td>\n",
       "      <td>0743509986</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Good Harbor</td>\n",
       "      <td>626222.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.23</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>https://s.gr-assets.com/assets/nophoto/book/11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7327624</td>\n",
       "      <td>8948723.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['189911']</td>\n",
       "      <td>The Unschooled Wizard (Sun Wolf and Starhawk, ...</td>\n",
       "      <td>10333.0</td>\n",
       "      <td>eng</td>\n",
       "      <td>4.03</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>https://images.gr-assets.com/books/1304100136m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6066819</td>\n",
       "      <td>6243154.0</td>\n",
       "      <td>0743294297</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Best Friends Forever</td>\n",
       "      <td>9212.0</td>\n",
       "      <td>eng</td>\n",
       "      <td>3.49</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>3282.0</td>\n",
       "      <td>51184.0</td>\n",
       "      <td>https://s.gr-assets.com/assets/nophoto/book/11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>287140</td>\n",
       "      <td>278577.0</td>\n",
       "      <td>0850308712</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Runic Astrology: Starcraft and Timekeeping in ...</td>\n",
       "      <td>149918.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>https://images.gr-assets.com/books/1413219371m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   book_id    work_id        isbn      series  \\\n",
       "0  5333265  5400751.0  0312853122         NaN   \n",
       "1  1333909  1323437.0  0743509986         NaN   \n",
       "2  7327624  8948723.0         NaN  ['189911']   \n",
       "3  6066819  6243154.0  0743294297         NaN   \n",
       "4   287140   278577.0  0850308712         NaN   \n",
       "\n",
       "                                               title   authors language_code  \\\n",
       "0                        W.C. Fields: A Life on Film  604031.0           NaN   \n",
       "1                                        Good Harbor  626222.0           NaN   \n",
       "2  The Unschooled Wizard (Sun Wolf and Starhawk, ...   10333.0           eng   \n",
       "3                               Best Friends Forever    9212.0           eng   \n",
       "4  Runic Astrology: Starcraft and Timekeeping in ...  149918.0           NaN   \n",
       "\n",
       "   average_rating  publication_year  text_reviews_count  ratings_count  \\\n",
       "0            4.00            1984.0                 1.0            3.0   \n",
       "1            3.23            2001.0                 6.0           10.0   \n",
       "2            4.03            1987.0                 7.0          140.0   \n",
       "3            3.49            2009.0              3282.0        51184.0   \n",
       "4            3.40               NaN                 5.0           15.0   \n",
       "\n",
       "                                           image_url  \n",
       "0  https://images.gr-assets.com/books/1310220028m...  \n",
       "1  https://s.gr-assets.com/assets/nophoto/book/11...  \n",
       "2  https://images.gr-assets.com/books/1304100136m...  \n",
       "3  https://s.gr-assets.com/assets/nophoto/book/11...  \n",
       "4  https://images.gr-assets.com/books/1413219371m...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the csv file\n",
    "df = pd.read_csv(csv_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find a book with multiple series\n",
    "# df[df['series'].str.contains(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
