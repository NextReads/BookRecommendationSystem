{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def readData():\n",
    "    df = pd.read_csv('dataset/IMDB Dataset.csv',header=0)\n",
    "    df.head()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nadou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nadou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nadou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\nadou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import nltk \n",
    "nltk.download('wordnet') \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nadou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Words Tokenization\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "def TokenizeWords(text):\n",
    "    review=[]\n",
    "    for i in text:\n",
    "        sent = re.sub(r'[^\\'\\w\\s]', ' ', i)\n",
    "        review.append(word_tokenize(sent.lower()))\n",
    "        # texts=word_tokenize(text.lower())\n",
    "    return review\n",
    "\n",
    "\n",
    "stopwords_english = stopwords.words('english')\n",
    "stopwords_english.append(\"<br />\")\n",
    "stopwords_english.append(\"br\")\n",
    "stopwords_english.append(\"<br /><br />\")\n",
    "stopwords_english.extend([\"'ll\",\"'re\",\"'ve\",\"'s\",\"'m\",\"n't\",\"'d\",\"'ll\",\"'re\",\"'ve\",\"'s\",\"'m\",\"n't\",\"'d\"])\n",
    "def removeSW(texts):\n",
    "    texts=[w for w in texts if not w.lower() in stopwords_english]\n",
    "    return texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into sentences for negative phrase identification\n",
    "def sentenceSplit(text):\n",
    "    text=text.replace('<br /><br />','').replace('(', ' ').replace(')', ' ')\n",
    "\n",
    "    sentences = re.split(r'[.!?,]', text)\n",
    "    # my_list_2 = re.split(r',|-|:|.', text)\n",
    "    sentences = [s.strip() for s in sentences if len(s) > 0]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy lemmatization\n",
    "\n",
    "import spacy\n",
    "# spacy.download('en')\n",
    "def spacyLemmatize(texts):\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    review=[]\n",
    "    for sent in texts:\n",
    "        doc = nlp(sent)\n",
    "        # texts= \" \".join([token.lemma_ for token in doc])\n",
    "        review.append(\" \".join([token.lemma_ for token in doc]))\n",
    "    return review\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "tags=[\"JJ\",\"JJR\",\"JJS\",\"RB\",\"RBR\",\"RBS\",\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\"]\n",
    "def pos_tagging(review):\n",
    "    pos_tag=[]\n",
    "    for sent in review:\n",
    "        tag=nltk.pos_tag(sent)\n",
    "        pos_tag.append(tag)\n",
    "    return pos_tag\n",
    "def TagNounremoval(review):\n",
    "    texts=[]\n",
    "    for sent in review:\n",
    "        text=[]\n",
    "        # print(sent)\n",
    "        for word in sent:\n",
    "            if word[1] in tags:\n",
    "                text.append(word)\n",
    "                \n",
    "        # print(text)\n",
    "        if len(text)>0:\n",
    "            texts.append(text) \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative Phrase Identification\n",
    "# we can add negative words to hashtable and then check if the word is present in the hash table later for better performance\n",
    "negative_words_english=[\"not\",\"n't\",\"no\",\"never\",\"none\",\"nothing\",\"nowhere\",\"neither\",\"nor\",\"hardly\",\"scarcely\",\"barely\",\"don't\",\"doesn't\",\"didn't\",\"isn't\",\"aren't\",\"wasn't\",\"weren't\",\"haven't\",\"hasn't\",\"hadn't\",\"won't\",\"wouldn't\",\"shan't\",\"shouldn't\",\"can't\",\"couldn't\",\"mustn't\",\"mightn't\",\"needn't\",\"oughtn't\",\"daren't\",\"needn't\",\"needn't've\",\"oughtn't've\",\"daren't've\",\"aren't\",\"aren't've\",\"couldn't\",\"couldn't've\",\"didn't\",\"didn't've\",\"doesn't\",\"doesn't've\",\"don't\",\"don't've\",\"hadn't\",\"hadn't've\",\"hasn't\",\"hasn't've\",\"haven't\",\"haven't've\",\"isn't\",\"isn't've\",\"mightn't\",\"mightn't've\",\"mustn't\",\"mustn't've\",\"needn't\",\"needn't've\",\"oughtn't\",\"oughtn't've\",\"shan't\",\"shan't've\",\"shouldn't\",\"shouldn't've\",\"wasn't\",\"wasn't've\",\"weren't\",\"weren't've\",\"won't\",\"won't've\",\"wouldn't\",\"wouldn't've\",\"mayn't\",\"might've\",\"must've\",\"needn't\",\"oughtn't\",\"sha'n't\",\"shan't\",\"shouldn't\",\"wasn't\",\"weren't\",\"won't\",\"wouldn't\",\"mustn't\",\"needn't\",\"oughtn't\",\"shan't\",\"shouldn't\",\"wasn't\",\"weren't\",\"won't\",\"wouldn't\",\"mustn't\",\"needn't\",\"oughtn't\",\"shan't\",\"shouldn't\",\"wasn't\",\"weren't\",\"won't\",\"wouldn't\",\"mustn't\",\"needn't\",\"oughtn't\",\"shan't\",\"shouldn't\",\"wasn't\",\"weren't\",\"won't\",\"wouldn't\",\"mustn't\",\"needn't\",\"oughtn't\",\"shan't\",\"shouldn't\",\"wasn't\",\"weren't\",\"won't\",\"wouldn't\",\"mustn't\",\"needn't\",\"oughtn't\",\"shan't\",\"shouldn't\",\"wasn't\",\"weren't\",\"won't\",\"wouldn't\",\"mustn't\",\"needn't\",\"oughtn't\",\"shan\"]\n",
    "def negPhraseIdentification(texts):\n",
    "    negPhrases=[]\n",
    "    for sentence in texts:\n",
    "        # print(sentence)\n",
    "        for word in sentence:\n",
    "            if word[0] in negative_words_english:\n",
    "                print(\"Negative: \",sentence)\n",
    "                break\n",
    "                    \n",
    "    return negPhrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contractions(s):\n",
    " s = re.sub(r\"won't\", \"will not\",s)\n",
    " s = re.sub(r\"would't\", \"would not\",s)\n",
    " s = re.sub(r\"could't\", \"could not\",s)\n",
    " s = re.sub(r\"\\'d\", \" would\",s)\n",
    " s = re.sub(r\"can\\'t\", \"can not\",s)\n",
    " s = re.sub(r\"n\\'t\", \" not\", s)\n",
    " s = re.sub(r\"\\'re\", \" are\", s)\n",
    " s = re.sub(r\"\\'s\", \" is\", s)\n",
    " s = re.sub(r\"\\'ll\", \" will\", s)\n",
    " s = re.sub(r\"\\'t\", \" not\", s)\n",
    " s = re.sub(r\"\\'ve\", \" have\", s)\n",
    " s = re.sub(r\"\\'m\", \" am\", s)\n",
    " return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingWord(review):\n",
    "    # review=contractions(review)\n",
    "    review = spacyLemmatize(review)\n",
    "    # print(review)\n",
    "    review=TokenizeWords(review)\n",
    "    review=removeSW(review)\n",
    "\n",
    "    \n",
    "    # review=lemmatize(review)\n",
    "    return review\n",
    "\n",
    "import re\n",
    "# text=\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\"\n",
    "# print(text)\n",
    "# print('--------------------------------')\n",
    "# print(preprocessingWord(text))\n",
    "# text=sentenceSplit(text)\n",
    "\n",
    "# text=pos_tagging(text)\n",
    "# negPhraseIdentification(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def sentMerge(review):\n",
    "    unzippedReview= list(chain(*review))\n",
    "    return unzippedReview\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePosTag(review):\n",
    "    texts=[]\n",
    "    for sent in review:\n",
    "        text=[]\n",
    "        for word in sent:\n",
    "            text.append(word[0])\n",
    "        texts.append(text)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization wordnet\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# review=[[('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('other', 'JJ'), ('reviewers', 'NNS'), ('has', 'VBZ'), ('mentioned', 'VBN'), ('that', 'IN'), ('after', 'IN'), ('watching', 'VBG'), ('just', 'RB'), ('1', 'CD'), ('oz', 'JJ'), ('episode', 'NN'), ('you', 'PRP'), (\"'ll\", 'MD'), ('be', 'VB'), ('hooked', 'VBN')], [('they', 'PRP'), ('are', 'VBP'), ('right', 'JJ')], [('as', 'IN'), ('this', 'DT'), ('is', 'VBZ'), ('exactly', 'RB'), ('what', 'WP'), ('happened', 'VBD'), ('with', 'IN'), ('me', 'PRP')], [('the', 'DT'), ('first', 'JJ'), ('thing', 'NN'), ('that', 'WDT'), ('struck', 'VBD'), ('me', 'PRP'), ('about', 'IN'), ('oz', 'NN'), ('was', 'VBD'), ('its', 'PRP$'), ('brutality', 'NN'), ('and', 'CC'), ('unflinching', 'JJ'), ('scenes', 'NNS'), ('of', 'IN'), ('violence', 'NN')], [('which', 'WDT'), ('set', 'VBP'), ('in', 'IN'), ('right', 'NN'), ('from', 'IN'), ('the', 'DT'), ('word', 'NN'), ('go', 'VB')], [('trust', 'NN'), ('me', 'PRP')], [('this', 'DT'), ('is', 'VBZ'), ('not', 'RB'), ('a', 'DT'), ('show', 'NN'), ('for', 'IN'), ('the', 'DT'), ('faint', 'NN'), ('hearted', 'VBD'), ('or', 'CC'), ('timid', 'VB')], [('this', 'DT'), ('show', 'NN'), ('pulls', 'VBZ'), ('no', 'DT'), ('punches', 'NNS'), ('with', 'IN'), ('regards', 'NNS'), ('to', 'TO'), ('drugs', 'NNS')], [('sex', 'NN'), ('or', 'CC'), ('violence', 'NN')], [('its', 'PRP$'), ('is', 'VBZ'), ('hardcore', 'NN')], [('in', 'IN'), ('the', 'DT'), ('classic', 'JJ'), ('use', 'NN'), ('of', 'IN'), ('the', 'DT'), ('word', 'NN')], [('it', 'PRP'), ('is', 'VBZ'), ('called', 'VBN'), ('oz', 'RB'), ('as', 'IN'), ('that', 'DT'), ('is', 'VBZ'), ('the', 'DT'), ('nickname', 'JJ'), ('given', 'VBN'), ('to', 'TO'), ('the', 'DT'), ('oswald', 'JJ'), ('maximum', 'JJ'), ('security', 'NN'), ('state', 'NN'), ('penitentary', 'NN')], [('it', 'PRP'), ('focuses', 'VBZ'), ('mainly', 'RB'), ('on', 'IN'), ('emerald', 'NNS'), ('city', 'NN')], [('an', 'DT'), ('experimental', 'JJ'), ('section', 'NN'), ('of', 'IN'), ('the', 'DT'), ('prison', 'NN'), ('where', 'WRB'), ('all', 'PDT'), ('the', 'DT'), ('cells', 'NNS'), ('have', 'VBP'), ('glass', 'NN'), ('fronts', 'NNS'), ('and', 'CC'), ('face', 'NN'), ('inwards', 'NNS')], [('so', 'RB'), ('privacy', 'NN'), ('is', 'VBZ'), ('not', 'RB'), ('high', 'JJ'), ('on', 'IN'), ('the', 'DT'), ('agenda', 'NN')], [('em', 'JJ'), ('city', 'NN'), ('is', 'VBZ'), ('home', 'VBN'), ('to', 'TO'), ('many', 'JJ')], [('aryans', 'NNS')], [('muslims', 'NNS')], [('gangstas', 'NNS')], [('latinos', 'NNS')], [('christians', 'NNS')], [('italians', 'NNS')], [('irish', 'JJ'), ('and', 'CC'), ('more', 'JJR')], [('so', 'RB'), ('scuffles', 'NNS'), ('death', 'NN'), ('stares', 'NNS'), ('dodgy', 'JJ'), ('dealings', 'NNS'), ('and', 'CC'), ('shady', 'JJ'), ('agreements', 'NNS'), ('are', 'VBP'), ('never', 'RB'), ('far', 'RB'), ('away', 'RB')], [('i', 'NN'), ('would', 'MD'), ('say', 'VB'), ('the', 'DT'), ('main', 'JJ'), ('appeal', 'NN'), ('of', 'IN'), ('the', 'DT'), ('show', 'NN'), ('is', 'VBZ'), ('due', 'JJ'), ('to', 'TO'), ('the', 'DT'), ('fact', 'NN'), ('that', 'IN'), ('it', 'PRP'), ('goes', 'VBZ'), ('where', 'WRB'), ('other', 'JJ'), ('shows', 'NNS'), ('would', 'MD'), (\"n't\", 'RB'), ('dare', 'VB')], [('forget', 'VB'), ('pretty', 'RB'), ('pictures', 'NNS'), ('painted', 'VBN'), ('for', 'IN'), ('mainstream', 'JJ'), ('audiences', 'NNS')], [('forget', 'VB'), ('charm', 'NN')], [('forget', 'VB'), ('romance', 'NN')], [('oz', 'NN'), (\"does\", 'VBZ'), (\"n't\", 'RB'), ('mess', 'VB'), ('around', 'RB')], [('the', 'DT'), ('first', 'JJ'), ('episode', 'NN'), ('i', 'NN'), ('ever', 'RB'), ('saw', 'VBD'), ('struck', 'VBN'), ('me', 'PRP'), ('as', 'IN'), ('so', 'RB'), ('nasty', 'JJ'), ('it', 'PRP'), ('was', 'VBD'), ('surreal', 'JJ'), ('i', 'NN'), ('could', 'MD')]]\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = tag[0].lower()\n",
    "    tag_dict = {\"j\": wordnet.ADJ,\n",
    "                \"n\": wordnet.NOUN,\n",
    "                \"v\": wordnet.VERB,\n",
    "                \"r\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "def lemmatize(review):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for sent in review:\n",
    "        # print(sent)\n",
    "        for i in range(len(sent)):\n",
    "            sent[i]=(lemmatizer.lemmatize(sent[i][0],pos=get_wordnet_pos(sent[i][1])),sent[i][1])\n",
    "        # print(sent)\n",
    "    return review\n",
    "# lemmatize(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reviewMerge(review):\n",
    "    return ' '.join(review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert text to string\n",
    "def convertToString(review):\n",
    "    review = str(review)\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "   \n",
    "    # print(data.iloc[:1])\n",
    "    data['review_text']=data['review_text'].apply(convertToString)\n",
    "    data['review_text']=data['review_text'].apply(sentenceSplit)\n",
    "    print(\"sentence split is complete\")\n",
    "    data['review_text']=data['review_text'].apply(TokenizeWords)\n",
    "    print(\"tokenization is complete\")\n",
    "    data['review_text']=data['review_text'].apply(pos_tagging)\n",
    "    # print(data['review_text'][0])\n",
    "    print(\"pos tagging is complete\")\n",
    "    data['review_text']=data['review_text'].apply(TagNounremoval)\n",
    "    print(\"noun removal is complete\")\n",
    "    data['review_text']=data['review_text'].apply(lemmatize)\n",
    "    print(\"lemmatization is complete\")\n",
    "    data['review_text']=data['review_text'].apply(removePosTag)\n",
    "    print(\"pos tag removal is complete\")\n",
    "    # print(data['review_text'][0])\n",
    "    data['review_text']=data['review_text'].apply(sentMerge)\n",
    "    print(\"merging is complete\")\n",
    "    # print(data['review_text'][0])\n",
    "    data['review_text']=data['review_text'].apply(removeSW)\n",
    "    print(\"stopword removal is complete\")\n",
    "    data['review_text']=data['review_text'].apply(reviewMerge)\n",
    "    print(\"review Merging is complete\")\n",
    "    # print(data['review'][0])\n",
    "    return data\n",
    "# print(df.iloc[:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "df=readData()\n",
    "preprocessing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e732d54d3afb58aaaf68d79121a18d7e534fb83a7a87b6bc6fe78fcf4e98b3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
